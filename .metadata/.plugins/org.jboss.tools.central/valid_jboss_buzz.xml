<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title type="html">How to run Java Mission Control in Eclipse</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-run-java-mission-control-in-eclipse/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-run-java-mission-control-in-eclipse/</id><updated>2022-07-27T15:02:27Z</updated><content type="html">This article continues our learning through the Java Mission Control (JMC) tool. Within it, we will learn how to run JMC as standalone application or as Eclipse IDE plugin. Firstly, if you are new to Java Mission Control, we recommend checking this article for a brief introduction to it: How to use Java Mission Control ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title>SaaS security in Kubernetes environments: A layered approach</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/27/saas-security-kubernetes-environments-layered-approach" /><author><name>Alex Kubacki</name></author><id>265fb1fc-a3c2-44d1-a2d0-41d140c6ac79</id><updated>2022-07-27T07:00:00Z</updated><published>2022-07-27T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/security/"&gt;Security&lt;/a&gt; is especially critical for &lt;a href="https://www.redhat.com/en/topics/cloud-computing/what-is-saas"&gt;Software-as-a-Service (SaaS)&lt;/a&gt; environments, where the platform is used by many different people who need the confidence that their data is stored safely and kept private from unrelated users. This article focuses on security concerns for &lt;a data-entity-substitution="canonical" data-entity-type="node" data-entity-uuid="4093cbb4-5a74-4678-9a7a-7e3d9c8b81c1" href="https://developers.redhat.com/topics/containers" title="Building containerized applications"&gt;containers&lt;/a&gt; on your SaaS deployment running in &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; environments such as &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The article is the fifth in a series called the &lt;a href="https://developers.redhat.com/articles/2022/05/18/saas-architecture-checklist-kubernetes"&gt;SaaS architecture checklist&lt;/a&gt; that covers the software and deployment considerations for SaaS applications.&lt;/p&gt; &lt;h2&gt;Security controls and practices for SaaS&lt;/h2&gt; &lt;p&gt;Within modern enterprise environments, security needs to be built into the full life cycle of planning, development, operations, and maintenance. Good security controls and practices are critical to meeting compliance and regulatory requirements and making sure that transactions are reliable and high-performing. Security in SaaS can be broken down into five main layers: hardware, operating system, containers, Kubernetes, and networking. Figure 1 shows these layers and the security controls that address threats at each layer.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/layers.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/layers.png?itok=5BHUmoFN" width="478" height="829" alt="SaaS layers and their security features in Kubernetes and OpenShift." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: SaaS layers and their security features in Kubernetes and OpenShift.&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Security needs to be addressed at every layer because any vulnerability in one layer could be exploited to compromise other layers. For each layer, Kubernetes and OpenShift have security controls and features that will be covered in this article. Future articles will go into more detail on specific SaaS security topics. If there are any SaaS topics for which you would like to see an article, let us know in the comments.&lt;/p&gt; &lt;h2&gt;Security at the hardware layer&lt;/h2&gt; &lt;p&gt;Securing a SaaS environment often starts with identifying where the application is going to run and the security concerns for that environment. A secure environment includes the actual data center as well as the hardware itself, including disk encryption, secure boot, BIOS-level passwords, and the use of hardware security modules (HSMs). Secrets and identity management are discussed later in this article.&lt;/p&gt; &lt;p&gt;While a lot of attention is paid to using encryption to protect &lt;em&gt;data in transit&lt;/em&gt; as it goes over the network, it is also critical to protect &lt;em&gt;data at rest&lt;/em&gt; as it is stored on physical storage devices in data centers. The risks to data at rest are much higher in data centers where you lack control over access to the facility and where third-party contractors may be employed. Use disk encryption to secure data at rest by protecting the data stored on the physical server from unintended access.&lt;/p&gt; &lt;p&gt;An HSM is typically a physical device that securely stores digital keys through encryption to protect sensitive data. HSMs are used to manage and safeguard security credentials, keys, certificates, and secrets while at rest and in transit. The HSM provides an increased level of protection over software-only approaches such as a secrets vault.&lt;/p&gt; &lt;p&gt;Cloud HSMs are available from the major cloud providers to provide increased protection in cloud environments. HSMs are recommended to manage secrets in SaaS environments.&lt;/p&gt; &lt;p&gt;Protect access to the server by enabling secure boot and using BIOS-level passwords. Secure boot is a firmware security feature of the Unified Extensible Firmware Interface (UEFI) that makes sure that only immutable and signed software can be run during boot.&lt;/p&gt; &lt;p&gt;For more information, check out:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/identity-and-access-devsecops-life-cycle"&gt;Identity and access in the DevSecOps life cycle&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://cloud.redhat.com/blog/self-contained-ready-and-secured-enhancing-red-hat-openshift-with-hardware-cryptography"&gt;Enhancing Red Hat OpenShift with Hardware Cryptography&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/8/html/security_hardening/assembly_securing-rhel-during-installation-security-hardening#BIOS_and_UEFI_security_securing-rhel-during-installation"&gt;Securing Red Hat Enterprise Linux during installation&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Operating system security&lt;/h2&gt; &lt;p&gt;Every Kubernetes cluster runs on top of some underlying operating system (OS). Security features and hardening at the OS layer help protect the overall cluster, so it is important to enable and use OS-level controls.&lt;/p&gt; &lt;p&gt;When it comes to security hardening at the OS level, Red Hat OpenShift has two distinct advantages. First, &lt;a href="https://www.redhat.com/en/topics/linux/what-is-selinux"&gt;Security-Enhanced Linux&lt;/a&gt; (SELinux) is integrated and enabled out of the box. Second, OpenShift runs on Red Hat Enterprise Linux CoreOS, a unique OS image tuned for SaaS use.&lt;/p&gt; &lt;h3&gt;Security-enhanced Linux&lt;/h3&gt; &lt;p&gt;SELinux is a security architecture for &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; systems that grants administrators finer-grained control over access to system resources than is available with default Linux. SELinux defines mandatory access controls for applications, processes, and files on a system. On a Kubernetes node, SELinux adds an important layer of protection against &lt;a href="https://www.redhat.com/en/blog/latest-container-exploit-runc-can-be-blocked-selinux"&gt;container-breakout vulnerabilities&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Thus, one of the most effective security measures is to enable and configure SELinux, which Red Hat has made standard on all OpenShift clusters. It is considered a best practice to use SELinux in SaaS environments. In OpenShift, SELinux enhances container security by ensuring true container separation and mandatory access control.&lt;/p&gt; &lt;p&gt;For more information, see:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/how-selinux-separates-containers-using-multi-level-security"&gt;How SELinux separates containers using Multi-Level Security&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/why-you-should-be-using-multi-category-security-your-linux-containers"&gt;Why you should be using Multi-Category Security for your Linux containers&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/using-container-technology-make-trusted-pipeline"&gt;Using container technology to make a more secure pipeline&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.redhat.com/en/blog/network-traffic-control-containers-red-hat-openshift"&gt;Network traffic control for containers in Red Hat OpenShift&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;h3&gt;A hardened OS for containers: Red Hat Enterprise Linux CoreOS&lt;/h3&gt; &lt;p&gt;OpenShift's operating system, Red Hat Enterprise Linux CoreOS, is based on Red Hat Enterprise Linux and uses the same kernel, code, and open source development processes. This special version ships with a specific subset of Red Hat Enterprise Linux packages, designed for use in OpenShift 4 clusters. The key features that make this operating system more secure are:&lt;/p&gt; &lt;ul&gt; &lt;li&gt; &lt;p&gt;Based on Red Hat Enterprise Linux: The underlying OS is primarily Red Hat Enterprise Linux components, which means it has the same quality, security, control measures, and support. When a fix is pushed to Red Hat Enterprise Linux, that same fix is pushed to Red Hat Enterprise Linux CoreOS.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Controlled immutability: Red Hat Enterprise Linux CoreOS is managed via OpenShift APIs, which leads to more hands-off operating system management. Management is primarily performed in bulk for all nodes throughout the OpenShift cluster. The latest state of the Red Hat Enterprise Linux CoreOS system is stored on the cluster, making it easy to add new nodes or push updates to all nodes. Given the OS's centralized management and transactional nature, only a few system settings can be modified on a Red Hat Enterprise Linux CoreOS installation.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Command-line container tools: Red Hat Enterprise Linux CoreOS includes container tools compatible with the &lt;a href="https://opencontainers.org"&gt;Open Container Initiative&lt;/a&gt; (OCI) specification to build, copy, and manage container images. Many container runtime administration features are available through Podman. The &lt;a href="https://www.redhat.com/sysadmin/how-run-skopeo-container"&gt;skopeo&lt;/a&gt; command copies, authenticates, and signs images. The &lt;a href="https://kubernetes.io/docs/tasks/debug/debug-cluster/crictl/"&gt;crictl&lt;/a&gt; command lets you view and troubleshoot containers and pods.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Robust transactional updates: Red Hat Enterprise Linux CoreOS offer the &lt;a href="https://coreos.github.io/rpm-ostree/"&gt;rpm-ostree&lt;/a&gt; upgrade process, which assures that an upgrade takes place atomically. If something goes wrong, the original OS can be restored in a single rollback.&lt;/p&gt; &lt;p&gt;OpenShift handles OS upgrades through the &lt;a href="https://github.com/openshift/machine-config-operator"&gt;Machine Config Operator&lt;/a&gt; (MCO), which encompasses a complete OS upgrade instead of individual packages as in traditional Yum upgrades. OpenShift also updates nodes via a rolling update to mitigate the updates' impact and maintain cluster capacity. During installation and upgrades, the latest immutable filesystem tree is read from a container image, written to disk, and loaded to the bootloader. The machine will reboot into the new OS version, guaranteeing an atomic update.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Security during cluster installation: Red Hat Enterprise Linux CoreOS minimizes security decisions during installation. Two security features are considered pre-first boot decisions for cluster operations: &lt;a href="https://docs.openshift.com/container-platform/4.8/installing/installing-fips.html#installing-fips"&gt;support for FIPS cryptography&lt;/a&gt; and full disk encryption (FDE). After the cluster is bootstrapped, the cluster can further be configured for other node-level changes.&lt;/p&gt; &lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Container layer&lt;/h2&gt; &lt;p&gt;The container layer in Kubernetes and OpenShift isolates processes from one another and from the underlying OS. Instead of traditional software design, where all the components are linked, deployed together, and ultimately dependent on each other, containers are independent, resulting in smaller impacts. If one container goes down, it can easily be replaced. If a container image is found to have a security flaw, the flaw is isolated to that image and requires updating only that image rather than the whole cluster.&lt;/p&gt; &lt;p&gt;Red Hat OpenShift has many features that improve container security for multitenant environments.&lt;/p&gt; &lt;h3&gt;Container engine&lt;/h3&gt; &lt;p&gt;A &lt;em&gt;container engine&lt;/em&gt; provides tools for creating container images and starting containers. In OpenShift, the default container engine is &lt;a href="https://docs.openshift.com/container-platform/3.11/crio/crio_runtime.html"&gt;CRI-O&lt;/a&gt;, which supports containers conforming to OCI and libcontainerd. The container engine focuses on the features needed by Kubernetes's &lt;a href="https://kubernetes.io/docs/concepts/architecture/cri/"&gt;Container Runtime Interface&lt;/a&gt; (CRI). This customized container engine shrinks the surface available to a security attack, because the container engine does not contain unneeded features such as direct command-line use or orchestration facilities.&lt;/p&gt; &lt;p&gt;We have also aligned the CRI more with Kubernetes: Updates to CRI-O are made to work better with the current Kubernetes release.&lt;/p&gt; &lt;h3&gt;Container security in the Linux kernel&lt;/h3&gt; &lt;p&gt;The kernel offers features to ensure the security of containers and everything else running on the OS. First off, all containers are launched inside a namespace that creates an isolated sandbox segregating the containers, files systems, processes, and networking.&lt;/p&gt; &lt;p&gt;The next feature is &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_enterprise_linux/6/html/resource_management_guide/ch01"&gt;control groups&lt;/a&gt; (cgroups), which isolate hardware resource sharing between containers and nodes of the OpenShift cluster. The use of cgroups prevents any single process or container from using up all the available resources on a host.&lt;/p&gt; &lt;p&gt;Finally, as we discussed earlier, Red Hat Enterprise Linux CoreOS enables SELinux, which prevents a container from breaking its isolation and thus interfering indirectly with other containers on the same host.&lt;/p&gt; &lt;h2&gt;Cluster security on Kubernetes and Red Hat OpenShift&lt;/h2&gt; &lt;p&gt;The cluster level controls how Kubernetes deploys hosts, manages shared resources, controls intercontainer communications, manages scaling, and controls access to the cluster. An OpenShift cluster is made up of a control plane, worker nodes, and any additional resources needed. The following subsections cover some of the security concerns for the different aspects of the cluster.&lt;/p&gt; &lt;h3&gt;Control plane isolation&lt;/h3&gt; &lt;p&gt;It is considered a best practice to isolate the cluster's control plane nodes from the worker nodes. This is usually done using separate hardware for the control plane to mitigate the impact of any misconfiguration, resource management problems, or vulnerabilities.&lt;/p&gt; &lt;h3&gt;Identity management&lt;/h3&gt; &lt;p&gt;Every Kubernetes cluster needs some form of identity management. Out of the box, Red Hat OpenShift comes with a default &lt;a href="https://oauth.net"&gt;OAuth&lt;/a&gt; provider, which is used for token-based authentication. This provider has a single &lt;code&gt;kubeadmin&lt;/code&gt; user account, which you can use to &lt;a href="https://docs.openshift.com/container-platform/4.10/authentication/understanding-identity-provider.html"&gt;configure an identity provider via a custom resource (CR)&lt;/a&gt;. OpenShift supports &lt;a href="https://openid.net/connect/"&gt;OpenID Connect&lt;/a&gt; and LDAP standard identity providers. After identities are defined, use role-based access control (RBAC) to define and apply permissions.&lt;/p&gt; &lt;h3&gt;Cluster access control&lt;/h3&gt; &lt;p&gt;Before users interact with the cluster, they first must authenticate via the OAuth server. Internal connections to the API server are authenticated using X.509 certificates.&lt;/p&gt; &lt;h3&gt;Security context constraints&lt;/h3&gt; &lt;p&gt;&lt;a href="https://docs.openshift.com/container-platform/4.8/authentication/managing-security-context-constraints.html"&gt;Security context constraints&lt;/a&gt; (SCCs) are an OpenShift security feature that limits a pod's resource access and allowable actions. SCCs let administrators control much of the pod's configuration, such as the SELinux context of a container, whether a pod can run privileged containers, and the use of host directories as volumes. In OpenShift, SCCs are enabled by default and cannot be disabled. SCCs can improve isolation in SaaS deployments and reduce the impact of potential vulnerabilities.&lt;/p&gt; &lt;p&gt;Pod SCCs are determined by the group that the user belongs to as well as the service account, if specified. By default, worker nodes and the pods running on them receive an SCC type of &lt;code&gt;restricted&lt;/code&gt;. This SCC type prevents pods from running as privileged and requires them to run under a UID that is selected at runtime from a preallocated range of UIDs.&lt;/p&gt; &lt;h3&gt;Secrets&lt;/h3&gt; &lt;p&gt;In SaaS deployments, the tenants need to secure their sensitive data on the cluster. This is handled with Secret objects on OpenShift. Secret objects hold sensitive information such as passwords, OCP client configuration files, private source repository credentials, etc. This way of using Secret objects decouples the sensitive content from the pods.&lt;/p&gt; &lt;p&gt;When the sensitive content is needed, it can be mounted to the container via a volume plugin, or the system can use the secrets to perform the action on behalf of the pod. Key properties of secrets include:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Secret data can be created by one entity, such as a configuration tool, and referred to by another, such as an application.&lt;/li&gt; &lt;li&gt;Secret data volumes are backed by temporary file-storage facilities (tmpfs) and never come to rest on a node.&lt;/li&gt; &lt;li&gt;Secret data can be shared within a namespace.&lt;/li&gt; &lt;li&gt;Secret data can &lt;a href="https://docs.openshift.com/container-platform/4.10/security/encrypting-etcd.html"&gt;optionally be encrypted at rest&lt;/a&gt;.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more information, read &lt;a href="https://docs.openshift.com/container-platform/4.10/nodes/pods/nodes-pods-secrets.html"&gt;Providing sensitive data to pods&lt;/a&gt;.&lt;/p&gt; &lt;h3&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/h3&gt; &lt;p&gt;In addition to the standard security features in Red Hat OpenShift, Red Hat offers additional products to enhance the security of the platform. One of those is &lt;a href="https://cloud.redhat.com/products/kubernetes-security"&gt;Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt; (previously StackRox). Red Hat Advanced Cluster Security for Kubernetes protects your vital applications across the build, deploy, and runtime stages. It deploys in your infrastructure and easily integrates with &lt;a href="https://developers.redhat.com/topics/devops/"&gt;DevOps&lt;/a&gt; tooling and workflows. This integration makes it easy to apply security and compliance policies.&lt;/p&gt; &lt;p&gt;Red Hat Advanced Cluster Security adds to OpenShift's built-in security by improving the following core tenants of security:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Improving &lt;em&gt;visibility&lt;/em&gt; of the environment, so administrators can more easily detect issues as they happen.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Managing vulnerabilities&lt;/em&gt; once they have been identified by deploying fixes via an integrated &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; pipeline.&lt;/li&gt; &lt;li&gt;Ensuring &lt;em&gt;compliance&lt;/em&gt; with industry standards and best practices.&lt;/li&gt; &lt;li&gt;Adding robust &lt;em&gt;network segmentation&lt;/em&gt; to restrict network traffic to only the necessary uses.&lt;/li&gt; &lt;li&gt;A &lt;em&gt;risk-based &lt;/em&gt;ranking of each deployment to determine the likelihood of a security risk, helping to ensure that the highest risk deployments get immediate remediation first.&lt;/li&gt; &lt;li&gt;Identifying misconfigurations and evaluating role-based access control (RBAC) access for users via &lt;em&gt;configuration management, &lt;/em&gt;to ensure that the configuration meets best practices.&lt;/li&gt; &lt;li&gt;&lt;em&gt;Runtime detection and response&lt;/em&gt; to automatically identify abnormal actions that could indicate a security breach or misuse of the environment.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;To learn more, see &lt;a href="https://cloud.redhat.com/blog/a-brief-introduction-to-red-hat-advanced-cluster-security-for-kubernetes"&gt;A Brief Introduction to Red Hat Advanced Cluster Security for Kubernetes&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Networking layer&lt;/h2&gt; &lt;p&gt;The networking layer is the outermost layer of a security architecture. The network is where most IT security attacks occur, due to misconfiguration and vulnerabilities. Proper planning and configuration of the network security layer components ensure that the environment is secure. Kubernetes has software-defined networking (SDN) controls that can improve network security in SaaS deployments. Red Hat OpenShift provides additional controls that build on what's available in Kubernetes.&lt;/p&gt; &lt;h3&gt;Network policy&lt;/h3&gt; &lt;p&gt;A network policy controls the traffic between pods by defining the permissions they need in order to communicate with other pods and network endpoints. OpenShift expands on policies by logically grouping components and rules into collections for easy management.&lt;/p&gt; &lt;p&gt;It is worth noting that network policies are additive. Therefore, when you create multiple policies on one or more pods, the union of all rules is applied regardless of the order in which you list them. The resulting pod behavior reflects every allow and deny rule for ingress and egress.&lt;/p&gt; &lt;h3&gt;Container network interface&lt;/h3&gt; &lt;p&gt;In a Kubernetes cluster, by default, pods are attached to a single network and have a single container network interface (CNI). The CNI manages the network connectivity of containers and removes resources when containers are deleted.&lt;/p&gt; &lt;p&gt;Kubernetes uses SDN plugins to implement the CNI. They manage the resources of the network interfaces for new pods. The CNI plugins set up proper networking constructs for pod-to-pod and pod-to-external communication and enforce network policies.&lt;/p&gt; &lt;h3&gt;Openshift networking security features&lt;/h3&gt; &lt;p&gt;OpenShift offers the following additional features and components to secure networks for cloud-native deployments:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Network operations: OpenShift includes a set of operators that manage networking components to enforce best practices and mitigate human errors.&lt;/li&gt; &lt;li&gt;Multiple network interfaces: The Kubernetes default is for all pods to use a single network and a single primary network interface, but with OpenShift, you can configure additional network interfaces. This allows network optimization to improve performance and enhances isolation to improve security.&lt;/li&gt; &lt;li&gt;Ingress security enhancements: OpenShift exposes the cluster to external resources or clients via a &lt;em&gt;route&lt;/em&gt; resource. Routes provide advanced features not found in a standard Kubernetes Ingress controller, including TLS re-encryption, TLS passthrough, and split traffic for blue-green deployments.&lt;/li&gt; &lt;li&gt;Egress security enhancements: While the default OpenShift rule allows all egress traffic to leave the cluster with no restrictions, OpenShift has tools for fine-grained control and filtering of outbound traffic. OpenShift lets you control egress traffic via an &lt;a href="https://docs.openshift.com/container-platform/4.6/networking/openshift_sdn/configuring-egress-firewall.html"&gt;egress firewall&lt;/a&gt;, &lt;a href="https://docs.openshift.com/container-platform/4.6/networking/openshift_sdn/using-an-egress-router.html"&gt;egress routers&lt;/a&gt;, and &lt;a href="https://docs.openshift.com/container-platform/4.7/networking/openshift_sdn/assigning-egress-ips.html"&gt;egress static IP addresses&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Service mesh: Red Hat OpenShift Service Mesh, based on the &lt;a href="https://istio.io"&gt;Istio&lt;/a&gt; project, adds a transparent layer to existing application network services running in a cluster, allowing complex management and monitoring without requiring changes to the services. The service mesh does this by deploying a sidecar proxy alongside the relevant services to intercept and manage all network communications. With Red Hat OpenShift Service Mesh, you can create a network with the following services: discovery, load balancing, service-to-service authentication, failure recovery, metrics, monitoring, A/B testing, canary releases, rate limiting, access control, and end-to-end authentication.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;For more information, see the &lt;a href="https://www.redhat.com/rhdc/managed-files/cl-openshift-security-guide-ebook-us287757-202103.pdf"&gt;Red Hat OpenShift Security Guide&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Partner with Red Hat to build your SaaS&lt;/h2&gt; &lt;p&gt;This article covered controls that can be used to improve the security of your SaaS deployment at the hardware, OS, container, Kubernetes cluster, and network levels. Future articles will go deeper into SaaS security topics.&lt;/p&gt; &lt;p&gt;&lt;a href="https://connect.redhat.com/en/partner-with-us/red-hat-saas-foundations"&gt;Red Hat SaaS Foundations&lt;/a&gt; is a partner program designed for building enterprise-grade SaaS platforms on Red Hat OpenShift or Red Hat Enterprise Linux, and deploying them across multiple cloud and non-cloud footprints. &lt;a href="http://https//mail.google.com/mail/?view=cm&amp;fs=1&amp;tf=1&amp;to=saas@redhat.com"&gt;Email&lt;/a&gt; us to learn more.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/27/saas-security-kubernetes-environments-layered-approach" title="SaaS security in Kubernetes environments: A layered approach"&gt;SaaS security in Kubernetes environments: A layered approach&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alex Kubacki</dc:creator><dc:date>2022-07-27T07:00:00Z</dc:date></entry><entry><title>Quarkus 2.11.1.Final and 2.10.4.Final released - Fixing CVE-2022-2466, new Redis Client API, more customization for some core extensions and more</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-2-11-1-final-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-2-11-1-final-released/</id><updated>2022-07-27T00:00:00Z</updated><published>2022-07-27T00:00:00Z</published><summary type="html">Today, we released both Quarkus 2.11.1.Final and Quarkus 2.10.4.Final. Quarkus 2.11.1.Final is, despite its name, the first 2.11 release. Both versions are fixing CVE-2022-2466 in the SmallRye GraphQL server extension (for real this time). Unfortunately, the previous fix introduced in 2.10.3.Final and the non-released 2.11.0.Final was incomplete and the issue...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2022-07-27T00:00:00Z</dc:date></entry><entry><title type="html">How to use Java Mission Control to monitor Java apps</title><link rel="alternate" href="http://www.mastertheboss.com/java/how-to-use-java-mission-control-to-monitor-java-apps/" /><author><name>F.Marchioni</name></author><id>http://www.mastertheboss.com/java/how-to-use-java-mission-control-to-monitor-java-apps/</id><updated>2022-07-25T07:37:43Z</updated><content type="html">This article is whirlwind tour across the Java Flight Recorder (JFR) and the Java Mission Control (JMC) suite. At the end of it, you will be able to monitor, collect diagnostic data and profile any running Java application. What is Java Flight Recorder? Firstly, why do we need another tool to monitor Java ? As ... The post appeared first on .</content><dc:creator>F.Marchioni</dc:creator></entry><entry><title type="html">RESTEasy Releases</title><link rel="alternate" href="https://resteasy.github.io/2022/07/21/resteasy-releases/" /><author><name /></author><id>https://resteasy.github.io/2022/07/21/resteasy-releases/</id><updated>2022-07-21T18:11:11Z</updated><dc:creator /></entry><entry><title>How to use Go Toolset container images</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images" /><author><name>Alejandro Sáez Morollón</name></author><id>011e5016-34ab-4570-8e1a-575cc4281eec</id><updated>2022-07-21T07:00:00Z</updated><published>2022-07-21T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_developer_tools/2019.1/html-single/using_go_toolset/index"&gt;Go Toolset package&lt;/a&gt; delivers the &lt;a href="https://developers.redhat.com/topics/go"&gt;Go language&lt;/a&gt; with Federal Information Processing Standard (FIPS) support for cryptographic modules and the &lt;a href="https://github.com/go-delve/delve"&gt;Delve debugger&lt;/a&gt; to &lt;a href="https://developers.redhat.com/products/rhel"&gt;Red Hat Enterprise Linux&lt;/a&gt; customers. We introduced this package &lt;a href="https://developers.redhat.com/blog/2017/10/31/getting-started-go-toolset"&gt;a few years ago&lt;/a&gt;. Now we also provide Go Toolset in container images. This article illustrates how these images support modern Go development.&lt;/p&gt; &lt;h2&gt;Obtaining Go Toolset container images&lt;/h2&gt; &lt;p&gt;The images are in Red Hat's &lt;a href="https://catalog.redhat.com/software/containers/explore"&gt;container image catalog&lt;/a&gt;. Search for &lt;code&gt;go-toolset&lt;/code&gt; &lt;a href="https://catalog.redhat.com/software/containers/search?q=go-toolset"&gt;here&lt;/a&gt;. As of this writing, we have images based on Red Hat Enterprise Linux 7, Red Hat Enterprise Linux 8, and &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat Universal Base Images&lt;/a&gt; (UBI) 7 and 8. To learn more about UBI, check the article &lt;a href="https://developers.redhat.com/products/rhel/ubi"&gt;Red Hat UBI is a Verified Publisher on Docker Hub&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;If you don't want to use the Go Toolset container image, you can still install the Go Toolset package inside a ubi8 container. Just run &lt;code&gt;dnf install go-toolset&lt;/code&gt; inside the &lt;code&gt;registry.access.redhat.com/ubi8&lt;/code&gt; image, and you'll be all set.&lt;/p&gt; &lt;h2&gt;Pull the latest version of the Go Toolset image&lt;/h2&gt; &lt;p&gt;Using your container engine is the best way to pull down a container image. Let's use &lt;a href="https://podman.io"&gt;Podman&lt;/a&gt; for this example. The following commands pull down the latest version of the image based on UBI 8 and run a shell inside it:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ podman pull registry.access.redhat.com/ubi8/go-toolset:latest [alex@lab ~]$ podman run --rm -it go-toolset /bin/bash bash-4.4$ go version go version go1.17.7 linux/amd64 bash-4.4$ exit exit [alex@lab ~]$ &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Using the image in a multistage environment&lt;/h2&gt; &lt;p&gt;You can refer to the image in a Dockerfile like any other image and use it, for example, as a build step in a multistage Dockerfile in tandem with the &lt;a href="https://catalog.redhat.com/software/containers/ubi8/ubi-micro/5ff3f50a831939b08d1b832a"&gt;Red Hat Universal Base Image 8 Micro image&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code class="Dockerfile"&gt;FROM ubi8/go-toolset as build COPY ./src . RUN go mod init my_app &amp;&amp; \ go mod tidy &amp;&amp; \ go build . FROM ubi8/ubi-micro COPY --from=build /opt/app-root/src/my_app . CMD ./my_app &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The ubi-micro image takes up less than 40MB, so the surface of your container is tiny but holds all of the great features UBI delivers.&lt;/p&gt; &lt;h2&gt;Go Toolset works with Toolbox&lt;/h2&gt; &lt;p&gt;I won’t be lying if I say &lt;a href="https://github.com/containers/toolbox"&gt;Toolbox&lt;/a&gt; is one of my favorite software tools. It allows you to keep working with your files and configurations inside a new container. I use Toolbox every day, and it works wonderfully with the Go Toolset image.&lt;/p&gt; &lt;p&gt;You can install Toolbox in both Red Hat Enterprise Linux and Fedora with &lt;code&gt;dnf install toolbox&lt;/code&gt;. The &lt;code&gt;toolbox&lt;/code&gt; command lets you install other resources:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ cat /etc/redhat-release Fedora release 35 (Thirty Five) [alex@lab ~]$ toolbox create --image registry.access.redhat.com/ubi8/go-toolset [alex@lab ~]$ toolbox enter go-toolset [alex@toolbox ~]$ cat /etc/redhat-release Red Hat Enterprise Linux release 8.6 (Ootpa) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you have all the files, configurations, and packages included with the image.&lt;/p&gt; &lt;h2&gt;Attach VS Code for IDE support&lt;/h2&gt; &lt;p&gt;You can improve productivity by attaching Visual Studio Code, &lt;a href="https://developers.redhat.com/products/vscode-extensions/overview"&gt;VS Code&lt;/a&gt; to a running container. Install the &lt;a href="https://marketplace.visualstudio.com/items?itemName=ms-vscode-remote.remote-containers"&gt;Remote Containers extension&lt;/a&gt; and execute &lt;strong&gt;Attach to Running Container&lt;/strong&gt;. There is no need to configure Git or Kerberos tokens; simply jump into the container and start working.&lt;/p&gt; &lt;p&gt;I use this process to play with the new versions of the container images we built and bootstrap projects such as Go.&lt;/p&gt; &lt;h2&gt;Go Toolset includes FIPS security&lt;/h2&gt; &lt;p&gt;One exciting feature supported by the golang package in Go Toolset is &lt;a href="https://www.redhat.com/en/blog/how-rhel-8-designed-fips-140-2-requirements"&gt;FIPS 140-2 cryptographic modules&lt;/a&gt;. You can expect Go Toolset to follow the FIPS 140-2 security standard. Check for FIPS mode inside the Go Toolset container image:&lt;/p&gt; &lt;pre&gt;&lt;code class="bash"&gt;[alex@lab ~]$ fips-mode-setup --check FIPS mode is enabled. [alex@lab ~]$ toolbox enter go-toolset [alex@toolbox ~]$ fips-mode-setup --check FIPS mode is enabled. &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Enterprise-ready Go in Red Hat Enterprise Linux&lt;/h2&gt; &lt;p&gt;The Go Toolset package is available as an image and integrates smoothly with other popular developer tools. By following the instructions I have provided, you can quickly become a more productive Go programmer in the cloud.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/21/how-use-go-toolset-container-images" title="How to use Go Toolset container images"&gt;How to use Go Toolset container images&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Alejandro Sáez Morollón</dc:creator><dc:date>2022-07-21T07:00:00Z</dc:date></entry><entry><title>Get started with OpenShift Service Registry</title><link rel="alternate" href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry" /><author><name>Evan Shortiss</name></author><id>fa17af9f-46e7-4502-8b24-a99e6890c3ed</id><updated>2022-07-20T16:00:00Z</updated><published>2022-07-20T16:00:00Z</published><summary type="html">&lt;p&gt;Red Hat OpenShift Service Registry is a fully hosted and managed service that provides an API and schema registry for &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt;. OpenShift Service Registry makes it easy for development teams to publish, discover, and reuse APIs and schemas.&lt;/p&gt; &lt;p&gt;Well-defined API and schema definitions are essential to delivering robust microservice and event streaming architectures. Development teams can use a registry to manage these artifacts in various formats, including &lt;a href="https://swagger.io/specification/"&gt;OpenAPI&lt;/a&gt;, &lt;a href="https://www.asyncapi.com/"&gt;AsyncAPI&lt;/a&gt;, &lt;a href="https://avro.apache.org/"&gt;Apache Avro&lt;/a&gt;, &lt;a href="https://developers.google.com/protocol-buffers"&gt;Protocol Buffers&lt;/a&gt;, and more. Data producers and consumers can then use the artifacts to validate and serialize or deserialize data.&lt;/p&gt; &lt;p&gt;This article gets you started with OpenShift Service Registry. You’ll create a &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;-based &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; application that uses the registry to manage schemas for data sent through topics in an &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt; cluster. The tutorial should take less than 30 minutes, and involves the following steps:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Create a Red Hat Hybrid Cloud account.&lt;/li&gt; &lt;li&gt;Provision an OpenShift Service Registry instance.&lt;/li&gt; &lt;li&gt;Provision an OpenShift Streams for Apache Kafka instance.&lt;/li&gt; &lt;li&gt;Create Kafka topics.&lt;/li&gt; &lt;li&gt;Create a service account to facilitate authenticated access to your Kafka and Service Registry instances.&lt;/li&gt; &lt;li&gt;Build and run a Java application.&lt;/li&gt; &lt;/ol&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Schemas and API definitions are metadata that represent a contract between decoupled services, so they must be discoverable, documented, and assigned versions to track their evolution over time.&lt;/p&gt; &lt;h2&gt;About OpenShift Service Registry&lt;/h2&gt; &lt;p&gt;Red Hat OpenShift Service Registry is based on the open source &lt;a href="https://www.apicur.io/registry/"&gt;Apicurio Registry project&lt;/a&gt;. It provides a highly available service registry instance that is secure and compatible with both the &lt;a href="https://docs.confluent.io/platform/current/schema-registry/develop/api.html"&gt;Confluent Schema Registry API&lt;/a&gt; and &lt;a href="https://github.com/cloudevents/spec/blob/master/schemaregistry/schemaregistry.md"&gt;CNCF Schema Registry API&lt;/a&gt;. OpenShift Service Registry is also a perfect companion service for applications that use &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://developers.redhat.com/products/red-hat-openshift-api-management/overview"&gt;Red Hat OpenShift API Management&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Prerequisites&lt;/h2&gt; &lt;p&gt;You need a Red Hat Hybrid Cloud account to run the examples in this article. Create an account for free at &lt;a href="https://console.redhat.com/"&gt;console.redhat.com&lt;/a&gt;. You also need the following tools in your development environment:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;Java 11 or higher&lt;/li&gt; &lt;li&gt;Maven 3.8.1 or higher&lt;/li&gt; &lt;li&gt;Git&lt;/li&gt; &lt;li&gt;Your favorite IDE or text editor&lt;/li&gt; &lt;/ul&gt; &lt;h2&gt;Creating a Service Registry instance&lt;/h2&gt; &lt;p&gt;Organizations or individuals with a Red Hat Hybrid Cloud account are entitled to a two-month trial instance of OpenShift Service Registry. To create an instance:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Log in to your account on &lt;a href="https://console.redhat.com"&gt;console.redhat.com&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;In the user interface (UI), select &lt;strong&gt;Application Services&lt;/strong&gt; from the menu on the left.&lt;/li&gt; &lt;li&gt;Expand the &lt;strong&gt;Service Registry&lt;/strong&gt; entry on the side menu and click the &lt;strong&gt;Service Registry Instances&lt;/strong&gt; link. Acknowledge the warning that it is a beta service.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create Service Registry instance&lt;/strong&gt; button. A modal dialog will be displayed.&lt;/li&gt; &lt;li&gt;Enter a name for your Service Registry instance and click the &lt;strong&gt;Create&lt;/strong&gt; button.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Your OpenShift Service Registry instance will be ready to use in a minute or two. A green checkmark will be displayed in the &lt;strong&gt;Status&lt;/strong&gt; column to indicate when the instance is ready, as shown in Figure 1.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/service_registry_ready.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/service_registry_ready.png?itok=2I9j3fwW" width="1440" height="807" alt="When a green checkmark and a Ready status are displayed in the OpenShift Service Registry UI, the Service Registry instance can be opened." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: A Service Registry instance listed in the OpenShift Service Registry UI. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt; &lt;/p&gt; &lt;p&gt;Once your instance is ready, click on its row in the portal to view connection information, as shown in Figure 2. Take note of the &lt;strong&gt;Core Registry API&lt;/strong&gt; value because you'll need it soon.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt; &lt;img alt="The service Registry instance connection information displayed in the UI." data-entity-type="file" data-entity-uuid="4e2e48ce-fc0e-417f-ab08-001c0c7068ee" src="https://developers.redhat.com/sites/default/files/inline-images/dblog-screen_0_0.png" width="1660" height="938" loading="lazy" /&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: The Service Registry instance connection information displayed in the UI.&lt;/figcaption&gt; &lt;/figure&gt; &lt;h2&gt;Integrating Java applications with Service Registry&lt;/h2&gt; &lt;p&gt;Kafka producer applications can use serializers to encode messages that conform to a specific event schema. Kafka consumer applications can then use deserializers to validate that messages were serialized using the correct schema, based on a specific schema ID. This process is illustrated in Figure 3. You'll test serialization and deserialization using Java producer and consumer applications that connect to Kafka.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/kafka_schema.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/kafka_schema.png?itok=8Rp6rq9H" width="1440" height="841" alt="Both producers and consumers in Kafka get schemas from the OpenShift Service Registry." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Kafka producer and consumer applications using the OpenShift Service Registry to share schemas. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h3&gt;Provision a managed Kafka instance and create topics&lt;/h3&gt; &lt;p&gt;To get started, you'll need to create an OpenShift Streams for Apache Kafka instance and two topics: one named &lt;code&gt;quote-requests&lt;/code&gt; and the other named &lt;code&gt;quotes&lt;/code&gt;. We've explained how to obtain this runtime environment for free in &lt;a href="https://developers.redhat.com/articles/2021/07/07/getting-started-red-hat-openshift-streams-apache-kafka#provision_a_kafka_cluster_with_openshift_streams_for_apache_kafka_through_the_ui"&gt;this article&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Remember to take note of your Kafka instance's bootstrap server URL. You will need this URL soon.&lt;/p&gt; &lt;h3&gt;Create a service account&lt;/h3&gt; &lt;p&gt;A service account is required to connect applications to the OpenShift Service Registry and OpenShift Streams for Apache Kafka instances. The service account provides a client ID and client secret that applications use to authenticate against the cloud services.&lt;/p&gt; &lt;p&gt;To create a service account:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Visit &lt;a href="https://console.redhat.com/beta/application-services/service-accounts"&gt;console.redhat.com/beta/application-services/service-accounts&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create service&lt;/strong&gt; account button.&lt;/li&gt; &lt;li&gt;Enter a name for the service account.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;The client ID and client secret will be displayed. Copy these down someplace safe.&lt;/li&gt; &lt;li&gt;Close the modal dialog.&lt;/li&gt; &lt;/ol&gt; &lt;h2&gt;Deploying the producer Java application&lt;/h2&gt; &lt;p&gt;At this point you have:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;An OpenShift Service Registry instance&lt;/li&gt; &lt;li&gt;An OpenShift Streams for Apache Kafka instance&lt;/li&gt; &lt;li&gt;A service account for connecting applications to the previous two instances&lt;/li&gt; &lt;li&gt;Kafka topics to hold messages published by a producer&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;Now, it's time to deploy a producer application that publishes messages to your Kafka topic. This application utilizes an Avro schema to encode messages in Avro format. It also publishes this schema to your OpenShift Service Registry. Consumer applications can fetch the schema from OpenShift Service Registry to deserialize and validate records they consume from your Kafka topic.&lt;/p&gt; &lt;p&gt;The source code for both the producer and the consumer is available in this &lt;a href="https://github.com/evanshortiss/rhosr-quarkus-kafka-apicurio"&gt;GitHub repository&lt;/a&gt;. Clone it into your development environment:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ git clone $REPOSITORY_URL rhosr-getting-started&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Open the &lt;code&gt;rhosr-getting-started&lt;/code&gt; project using your preferred IDE or text editor, and open the &lt;code&gt;producer/pom.xml&lt;/code&gt; file. This file contains typical dependencies that are used to connect to Kafka and expose REST services. The &lt;code&gt;quarkus-apicurio-registry-avro&lt;/code&gt; dependency is used to generate Java classes based on Avro schema definitions. It also brings in dependencies required to work with the service registry, such as service registry-aware Kafka serializers and deserializers.&lt;/p&gt; &lt;p&gt;Next, open the &lt;code&gt;producer/src/main/avro/quote.avsc&lt;/code&gt; file. This file contains an Avro schema defined using JSON. This schema can be used to generate a &lt;code&gt;Quote.java&lt;/code&gt; class that extends and implements the necessary Avro class and interface. The &lt;code&gt;Quote&lt;/code&gt; class is used to serialize outgoing messages to the underlying Kafka topic, and by the &lt;code&gt;quotes&lt;/code&gt; channel to deserialize incoming messages. The generated class can be found in the &lt;code&gt;target/generated-sources/Quota.java&lt;/code&gt; file after compiling the application or running it in development mode.&lt;/p&gt; &lt;p&gt;Lastly, examine the &lt;code&gt;producer/src/main/resource/application.properties&lt;/code&gt; file. This file configures the application to connect to a Kafka instance, register schemas with a registry, and use Avro serialization and deserialization.&lt;/p&gt; &lt;h3&gt;Run the producer application&lt;/h3&gt; &lt;p&gt;You can run the producer application wherever you like, including on an OpenShift cluster. I'll demonstrate how you can run the producer in your local development environment.&lt;/p&gt; &lt;p&gt;First, define the following environment variables in a shell. Replace the text within &lt;strong&gt;&lt;&gt;&lt;/strong&gt; angle brackets with the values you found in previous sections:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Used to authenticate against the registry and kafka cluster export CLIENT_ID=&lt;your-client-id&gt; export CLIENT_SECRET=&lt;your-client-secret&gt; # Used to connect to and authenticate against the service registry export OAUTH_SERVER_URL=https://sso.redhat.com/auth export REGISTRY_URL=&lt;core-service-registry-url&gt; # Used to connect to and authenticate against the kafka cluster export BOOTSTRAP_SERVER=&lt;kafka-bootstrap-url&gt; export OAUTH_TOKEN_ENDPOINT_URI=https://sso.redhat.com/auth/realms/redhat-external/protocol/openid-connect/token&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Once these values are defined, you can start the producer application in the same shell using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ mvn quarkus:dev -f ./producer/pom.xml -Dquarkus-profile=prod&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Quarkus application is now running in and has exposed an HTTP server on &lt;a href="http://localhost:8080/"&gt;http://localhost:8080/&lt;/a&gt;. Use the following command to send a POST request that creates a quote and sends it to the &lt;code&gt;quote-requests&lt;/code&gt; Kafka topic:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ curl -X POST http://localhost:8080/quotes/request&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You should receive a response in JSON format that contains a unique quote &lt;code&gt;id&lt;/code&gt; and random &lt;code&gt;price&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;View the Quote schema in Service Registry&lt;/h3&gt; &lt;p&gt;When you start the producer application and make a request to the &lt;code&gt;/quotes/request&lt;/code&gt; endpoint, the producer gets ready to send data to your Kafka topic. Prior to sending the data, the producer checks that the Quote Avro schema is available in OpenShift Service Registry. If the Quote schema is not found, the producer publishes the schema to the registry. The producer then serializes the outgoing data using the schema and includes the registered schema ID in the message value.&lt;/p&gt; &lt;p&gt;A downstream consumer application can use the schema ID found in the message payload to fetch the necessary schema from the registry. The consumer application can then use the schema to validate and deserialize the incoming message.&lt;/p&gt; &lt;p&gt;To confirm that the Avro schema was published to OpenShift Service Registry:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Go to your &lt;a href="https://console.redhat.com/beta/application-services/service-registry"&gt;OpenShift Service Registry Instances&lt;/a&gt; listing.&lt;/li&gt; &lt;li&gt;Select the instance used by your producer application.&lt;/li&gt; &lt;li&gt;Select the &lt;strong&gt;Artifacts&lt;/strong&gt; tab.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;You should see the Quote schema, as shown in Figure 4.&lt;/p&gt; &lt;figure role="group"&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/quote_schema.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/quote_schema.png?itok=6Gj9pQGo" width="1440" height="810" alt="The Quote Avro schema is listed in the OpenShift Service Registry." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: The Quote Avro schema listed in OpenShift Service Registry. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt; &lt;/figure&gt; &lt;p&gt;Select the Quote schema in the list and view the &lt;strong&gt;Content&lt;/strong&gt; tab. Use the &lt;strong&gt;Format&lt;/strong&gt; button to improve the legibility of the JSON, and confirm that it matches the &lt;code&gt;Quote.avsc&lt;/code&gt; file in the producer application codebase.&lt;/p&gt; &lt;h2&gt;Consuming messages in Service Registry&lt;/h2&gt; &lt;p&gt;The repository you cloned as part of this exercise contains a consumer application. This consumer application is configured using the same environment variables as the producer and reads messages from the &lt;code&gt;quote-requests&lt;/code&gt; topic. Because the producer and consumer use OpenShift Service Registry, the consumer can fetch the necessary Avro schema to validate and deserialize incoming quote requests.&lt;/p&gt; &lt;p&gt;Run the producer and consumer at the same time. Use cURL to open a connection to the server-sent events (SSE) endpoint at &lt;a href="http://localhost:8080/quotes"&gt;http://localhost:8080/quotes&lt;/a&gt;, then use another HTTP client to POST to &lt;a href="http://localhost:8080/quotes/request"&gt;http://localhost:8080/quotes/request&lt;/a&gt;. The consumer should correctly deserialize and process the items from the &lt;code&gt;quote-requests&lt;/code&gt; topic and place the processed quote into the &lt;code&gt;quotes&lt;/code&gt; topic, after which the SSE endpoint should display the items as shown in Figure 5.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/curl.jpeg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/curl.jpeg?itok=FEaLwNB0" width="1440" height="796" alt="A cURL command displays deserialized, JSON-formatted data received by the consumer at the SSE endpoint." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Deserialized, JSON-formatted data received by the consumer at the SSE endpoint and displayed by a cURL command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;h2&gt;Enforcing schema compatibility rules&lt;/h2&gt; &lt;p&gt;OpenShift Service Registry supports various schema compatibility rules to prevent the publication of schema changes that could lead to incompatibilities with downstream applications (that is, breaking changes). You can read more about compatibility rules in the &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_openshift_service_registry/1/guide/9b0fdf14-f0d6-4d7f-8637-3ac9e2069817"&gt;service documentation&lt;/a&gt;. To enable this enforcement:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Open the Service Registry UI at &lt;a href="https://console.redhat.com/beta/application-services/service-registry"&gt;console.redhat.com/beta/application-services/service-registry&lt;/a&gt;.&lt;/li&gt; &lt;li&gt;Select your instance and view the Quote schema.&lt;/li&gt; &lt;li&gt;Set the &lt;strong&gt;Validity Rule&lt;/strong&gt; to &lt;strong&gt;Full&lt;/strong&gt; and the &lt;strong&gt;Compatibility Rule&lt;/strong&gt; to &lt;strong&gt;Backward&lt;/strong&gt; (see Figure 6).&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Upload new version&lt;/strong&gt; button.&lt;/li&gt; &lt;li&gt;Paste in the following Avro schema and click &lt;strong&gt;Upload&lt;/strong&gt;:&lt;/li&gt; &lt;/ol&gt; &lt;pre&gt; &lt;code&gt;{ "namespace": "org.acme.kafka.quarkus", "type": "record", "name": "Quote", "fields": [ { "name": "id", "type": "string" }, { "name": "price", "type": "int" }, { "name": "notes", "type": "string" } ] }&lt;/code&gt; &lt;/pre&gt; &lt;p&gt;Figure 6 shows these updates in the console.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt; &lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/rule.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/rule.png?itok=j6E6m_0t" width="1440" height="817" alt="The Compatibility Rule can be set in the OpenShift Service Registry UI." loading="lazy" typeof="Image" /&gt; &lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Enforcing schema compatibility rules using the OpenShift Service Registry UI. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt; &lt;/div&gt; &lt;p&gt;An &lt;strong&gt;Invalid Content&lt;/strong&gt; error should be displayed, because this new schema violated the backward compatibility rule by adding a new required field. New fields must be optional if backward compatibility is enabled. As the error message indicated, new schemas are required to provide backward-compatible schemas for all future evolution.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Congratulations—in this article you have learned how to:&lt;/p&gt; &lt;ol&gt; &lt;li&gt;Use OpenShift Service Registry.&lt;/li&gt; &lt;li&gt;Use OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Create Avro schemas.&lt;/li&gt; &lt;li&gt;Integrate Java applications that use Avro schemas with both services.&lt;/li&gt; &lt;li&gt;Manage schema evolution and apply rules to prevent breaking changes for downstream consumers.&lt;/li&gt; &lt;/ol&gt; &lt;p&gt;Sign up for the services described in this article, and let us know your experience in the comments.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/10/11/get-started-openshift-service-registry" title="Get started with OpenShift Service Registry"&gt;Get started with OpenShift Service Registry&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2022-07-20T16:00:00Z</dc:date></entry><entry><title type="html">An improved &amp;#8216;Spreadsheet like&amp;#8217; experience on DMN Editor</title><link rel="alternate" href="https://blog.kie.org/2022/07/a-better-spreadsheet-like-experience.html" /><author><name>Fabrizio Antonangeli</name></author><id>https://blog.kie.org/2022/07/a-better-spreadsheet-like-experience.html</id><updated>2022-07-20T14:33:15Z</updated><content type="html">The boxed expression editor is a key component of the DMN Editor. In previous articles, we introduced the new implementation of this . In this article, I’ll show how I extended the component, implementing the keyboard navigation for faster DMN editing. REQUIREMENTS * (1.46.0+); * (0.20.0+); , there is a ready-to-use online version of the DMN editor to try the new functionality. THE NEW KEYBOARD NAVIGATION The editing of a decision in the DMN Editor was based on the mouse interaction requiring the user to continuously switch between keyboard and mouse, resulting in a time-consuming activity. As my first task on the project, I worked on implementing a user experience as much as possible similar to Google Spreadsheet. As a result, the user can edit an expression, cell by cell, seamlessly using only the keyboard. THE NEW KEYBOARD ACTIONS AVAILABLE IN “VIEW MODE” Navigation between cells is now available in any type of expression using the arrow keys to go UP, RIGHT, DOWN, and LEFT, in a natural way. Continuous navigation is available with TAB, to jump to the next cell, or SHIFT-TAB to jump to the previous cell. This with the exception that if you are at the end of the row, you jump to the first cell of the next row or the last cell of the previous row if you are on the first cell of the row. After you choose the cell you want to edit, you just start writing the content, and this way, you erase the already existing content, if there is any. In addition, if you just want to start editing from the end of the cell’s content, you just need to press ENTER and start typing your content. Differently from a normal Spreadsheet, we have particular cells with nested tables or cells that don’t have just text, and when you click on them, a pop-up with a few inputs will appear, used in different cases. In this last case, you can now open the pop-up by pressing ENTER, and then you jump between the inputs inside the form using TAB/SHIFT-TAB. ENTER/ESC will close the pop-up and save or cancel your changes. For the case of nested tables, for instance, a "Context expression" with a Decision Table inside, you can jump inside the nested table with ENTER key and come back to the parent table with ESC. THE NEW KEYBOARD ACTIONS AVAILABLE IN “EDIT MODE” When you finish editing a cell, and you want to apply your changes, using TAB/SHIFT-TAB you save and jump directly to the next or previous cell, or to the cell below with ENTER. On the contrary, you can press ESC to cancel your changes to the cell. A big change to the UX was the introduction of the "newline" in the Decision Table’s cells, using the CTRL-ENTER. This change impacted the logic of the "copy &amp;amp; paste" that was based on single-line cells. To achieve that, the new logic is based on the parser, and now you can just copy your data to or from a Spreadsheet. The implementation The work has been mainly focused on the package inside repository. The main obstacle to implementing all these functionalities was the communication between React custom and third-party components and highlighting the selected cell. The easiest way was possibly the use of the “contenteditable” attribute, but that required a full rewriting of the components for the table and cells. After an evaluation of 4 other solutions, we decided to listen to the keyboard events from the TD HTML element representing the cell, then show the highlight through the “:focus” CSS selector. This way managing the "onBlur" event or memorizing the selected cell is not needed. Instead, when the focus is on the input, which is actually a Monaco editor, the highlight needs to be on the component inside the cell, that has the state data. This is done by adding the CSS class "editable-cell–edit-mode" to the main tag of the component EditableCell. Then to ensure the stability of the component we use "Jest" and "@testing-library/react" to render components. For the E2E test, we use "Cypress", which currently doesn’t support TAB key simulation which we managed with the plugin. CONCLUSION Creating or modifying a Decision can be a time-consuming activity if you have a lot of data to add inside. Giving the user the ability to write that data quickly and in the way, he was used to, was very important to us. We also didn’t want the user to learn a new way for that, and we wanted to give the same experience as Google Spreadsheet or Excel. The post appeared first on .</content><dc:creator>Fabrizio Antonangeli</dc:creator></entry><entry><title>Git workflows: Best practices for GitOps deployments</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments" /><author><name>Christian Hernandez</name></author><id>9298f676-ac50-4d48-b863-304eb4a1e95b</id><updated>2022-07-20T07:00:00Z</updated><published>2022-07-20T07:00:00Z</published><summary type="html">&lt;p class="Indent1"&gt;&lt;strong&gt;Note: &lt;/strong&gt;This is an excerpt from the &lt;em&gt;The Path to GitOps&lt;/em&gt;. &lt;a href="https://developers.redhat.com/e-books/path-gitops"&gt;Download the e-book today&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Your Git workflows are at the center of your &lt;a href="https://developers.redhat.com/topics/gitops"&gt;GitOps&lt;/a&gt; deployments because workflows are the means of implementing your changes in your environment. When you adopt GitOps, Git is not only your source of truth (as it is for most projects) but also your interface into your environment. Developers have used Git workflows for their application delivery method for years, and now operations teams will have to adopt similar workflows.&lt;/p&gt; &lt;p&gt;But there are key differences between how you manage your code in Git and how you manage your GitOps configuration in Git. Here I will go over these differences and describe the best practices you should follow to make the best use of Git workflows for your GitOps deployments. We will see how to separate your configuration from your code, how to use branches, how to use trunk-based development workflows effectively, and tips for setting up policies and security for your Git workflows.&lt;/p&gt; &lt;h2&gt;Separate your repositories&lt;/h2&gt; &lt;p&gt;There are a few things to keep in mind when setting up your Git workflows for your GitOps directory structure and GitOps in general. The first is to keep your application code in a separate repository from your YAML configurations. This might seem counterintuitive initially, but most teams that start with code and configurations together quickly learn it’s better to separate them. &lt;/p&gt; &lt;p&gt;There are a few reasons for separate repositories. First, you don’t want a configuration change (such as changing the scale of a deployment from three to four nodes) to trigger a rebuild of your application if your application code didn’t change. Another reason is that the approval process of getting a change into an environment shouldn’t hold back continuous integration of your code. In general, application code and configuration information have independent lifecycles.&lt;/p&gt; &lt;p&gt;Also, many organizations separate the deployment process into several different teams. A lot of the time, the operations or release management team takes care of the application’s release. Although DevOps aims to reduce barriers between teams and their activities, you don’t want one team’s process to slow down another.&lt;/p&gt; &lt;h2&gt;Separate development in directories, not branches&lt;/h2&gt; &lt;p&gt;Another best practice that surprises many programmers is to separate environments–such as test and production–into different directories, but not create branches for them. Like the separation of code and configurations, this principle might seem to go against the grain of version control, but keeping track of environmental branches can be a challenge.&lt;/p&gt; &lt;p&gt;One of the difficulties you might encounter if you manage workflows through branches is that promotion from one environment to another isn’t as simple as a merge. You can see this issue with a simple example of updating an image tag. The application has been built, tested, and deemed ready to go from a sandbox environment to a test environment. But updating the image tag comes with other changes you don’t want to merge. What about the scale in the &lt;a href="https://kubernetes.io/docs/concepts/workloads/controllers/deployment/"&gt;Deployment&lt;/a&gt;? What about the ConfigMaps and Secrets? Those are bound to change in different environments and include things that should not be merged into other environments.&lt;/p&gt; &lt;p&gt;In short, every environment has configuration details specific to that environment. You can manually make the changes one by one or &lt;a href="https://git-scm.com/docs/git-cherry-pick"&gt;"cherry-pick"&lt;/a&gt;, but then your "simple merge" is no longer that simple. When you’re constantly cherry-picking or making manual changes, the effort level outweighs the benefits of trying to mirror the application workflows.&lt;/p&gt; &lt;p&gt;Another danger of using branches and cherry-picking your way into production is that this will likely introduce a significant drift. As you get further along in the life of a software project, when it spawns hundreds of environments with dozens upon dozens of applications, you can quickly see how cherry-picking and making manual changes can get out of hand. You can no longer use &lt;a href="https://git-scm.com/docs/git-diff"&gt;a simple diff&lt;/a&gt; to see the differences between branches, as the differences will be astronomical. &lt;/p&gt; &lt;p&gt;In the world of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;, the &lt;a href="https://kustomize.io"&gt;Kustomize&lt;/a&gt; patching framework, and the &lt;a href="https://helm.sh"&gt;Helm&lt;/a&gt; package manager, using branches for environments is an antipattern. Kustomize and Helm make using directories and overlays for your environments easier. Kustomize, in particular, allows you to have a core set of manifests (called a "base" in Kustomize) and store the deltas in directories (called "overlays" in Kustomize). You use these overlays as directories with specific environment configurations in these directories.&lt;/p&gt; &lt;p&gt;So do you use branches at all? Yes, but not in the way you think. With GitOps, trunk-based development has emerged as the development model for your configuration repositories.&lt;/p&gt; &lt;h2&gt;Trunk-based development&lt;/h2&gt; &lt;p&gt;The recommended workflow for implementing GitOps with Kubernetes manifests is known as &lt;a href="https://trunkbaseddevelopment.com/"&gt;trunk-based development&lt;/a&gt;. This method defines one branch as the "trunk" and carries out development on each environment in a different short-lived branch. When development is complete for that environment, the developer creates a pull request for the branch to the trunk. Developers can also create a fork to work on an environment, and then create a branch to merge the fork into the trunk.&lt;/p&gt; &lt;p&gt;Once the proper approvals are done, the pull request (or the branch from the fork) gets merged into the trunk. The branch for that feature is deleted, keeping your branches to a minimum. Trunk-based development trades branches for directories.&lt;/p&gt; &lt;p&gt;You can think of the trunk as a "main" or primary branch. production and prod are popular names for the trunk branch.&lt;/p&gt; &lt;p&gt;Trunk-based development came about to enable continuous integration and continuous delivery by supplying a development model focused on the fast delivery of changes to applications. But this model also works for GitOps repositories because it keeps things simple and more in tune with how Kustomize and Helm work. When you record deltas between environments, you can clearly see what changes will be merged into the trunk. You won’t have to cherry-pick nearly as often, and you’ll have the confidence that what is in your Git repository is what is actually going into your environment. This is what you want in a GitOps workflow.&lt;/p&gt; &lt;h2&gt;Pay attention to policies and security&lt;/h2&gt; &lt;p&gt;Part of the challenge with trunk-based development is that now there is a single branch where things can go wrong. When relying on Git as your source of truth, it can be quite scary to depend on a single branch for not only your production environment but your organization as a whole. So you need to pay special attention to the features that version control offers for policy management and security to protect your trunk and provide stability to your environment.&lt;/p&gt; &lt;p&gt;When setting up your Git repository policies, use &lt;a href="https://docs.github.com/en/repositories/configuring-branches-and-merges-in-your-repository/defining-the-mergeability-of-pull-requests/about-protected-branches#about-branch-protection-rules"&gt;GitHub’s branch protection rules&lt;/a&gt; (or the equivalent from other Git providers). Setting branch protection rules provides several benefits, the most important of which is preventing someone from force pushing a change into the trunk (which in turn makes an immediate alteration to your environment). Branch protection also protects the branch from being accidentally or intentionally deleted. There are other advantages to protected branches, but the main takeaway is this: You need to trust what is in Git because it is in charge of managing your environment. Take every precaution that builds trust.&lt;/p&gt; &lt;p&gt;Also, set up rules as to who can perform a merge and when. Make sure that all affected parties in your organization see a proposed merge. For example, perhaps a network change should be approved not only by the system administration team but also by the networking team and the security team. A rule can take the form of a "minimum number of approvals," but that doesn’t limit the number of approvals to the minimum. And while you’ll have multiple approvers, you should allow only a handful of people to actually merge the change.&lt;/p&gt; &lt;h2&gt;Next steps&lt;/h2&gt; &lt;p&gt;Read &lt;em&gt;&lt;a href="https://developers.redhat.com/e-books/path-gitops"&gt;The Path to GitOps&lt;/a&gt; &lt;/em&gt;to discover where GitOps fits in your CI/CD (continuous integration/continuous delivery) pipelines and explore the various ways you can implement it. You'll learn about popular tools like Argo CD and Flux and see how Kustomize, Helm, and Kubernetes Operators make it easier to deal with lengthy configuration files.&lt;/p&gt; &lt;p&gt;Find even more GitOps resources from Red Hat Developer:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;&lt;em&gt;&lt;a href="https://developers.redhat.com/e-books/getting-gitops-practical-platform-openshift-argo-cd-and-tekton"&gt;Getting GitOps: A practical platform with OpenShift, Argo CD, and Tekton&lt;/a&gt;&lt;/em&gt; helps you put it all together by walking through a common use case from beginning to end.&lt;/li&gt; &lt;li&gt;Get a preview of &lt;em&gt;&lt;a href="https://developers.redhat.com/e-books/gitops-cookbook"&gt;GitOps Cookbook&lt;/a&gt;&lt;/em&gt;, a collection of useful recipes to follow GitOps practices on Kubernetes.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/20/git-workflows-best-practices-gitops-deployments" title="Git workflows: Best practices for GitOps deployments"&gt;Git workflows: Best practices for GitOps deployments&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Christian Hernandez</dc:creator><dc:date>2022-07-20T07:00:00Z</dc:date></entry><entry><title>Secure Kubernetes certificates with cert-manager and Dekorate</title><link rel="alternate" href="https://developers.redhat.com/articles/2022/07/19/secure-kubernetes-certificates-cert-manager-and-dekorate" /><author><name>Jose Carvajal Hilario, Ana-Maria Mihalceanu, Charles Moulliard</name></author><id>6a78a218-d650-4212-b182-8982603d1964</id><updated>2022-07-19T07:00:00Z</updated><published>2022-07-19T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://cert-manager.io/"&gt;Cert-manager&lt;/a&gt; is a cloud-native certificate management service for &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; and &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. To configure cert-manager, you need to install several resources using custom resource definitions (CRDs). Depending on the issuer type and the certificate you need, creating these custom resources can become complex. This article introduces &lt;a href="https://dekorate.io/"&gt;Dekorate&lt;/a&gt; as an easier way to generate the cert-manager custom resources. We will also provide an example &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; application based on &lt;a href="https://developers.redhat.com/topics/spring-boot"&gt;Spring Boot&lt;/a&gt; that uses the certificate generated by cert-manager.&lt;/p&gt; &lt;h2&gt;Getting started with the Dekorate cert-manager extension&lt;/h2&gt; &lt;p&gt;Cert-manager requires the installation of several resources, including &lt;code&gt;Issuer&lt;/code&gt;, &lt;code&gt;ClusterIssuer&lt;/code&gt;, and &lt;code&gt;Certificate&lt;/code&gt;. Cert-manager processes these resources to populate a secret, containing authentication information, such as a CA certificate, private key, server certificate, or Java keystores. This secret can then be used to secure your application endpoints or &lt;a href="https://kubernetes.io/docs/concepts/services-networking/ingress/"&gt;Kubernetes Ingress&lt;/a&gt; resources.&lt;/p&gt; &lt;p&gt;Dekorate, starting with version 2.10, can generate the certificate and issuer resources for you. Include the cert-manager Dekorate dependency in your POM file using the latest version of Dekorate from &lt;a href="https://search.maven.org/search?q=a:certmanager-annotations%20AND%20g:io.dekorate"&gt;Maven central&lt;/a&gt; as follows:&lt;/p&gt; &lt;pre&gt;&lt;code class="xml"&gt;&lt;dependency&gt; &lt;groupId&gt;io.dekorate&lt;/groupId&gt; &lt;artifactId&gt;certmanager-annotations&lt;/artifactId&gt; &lt;version&gt;{dekorate.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The minimal information Dekorate needs for certificate configuration is:&lt;/p&gt; &lt;ul&gt; &lt;li&gt;A &lt;code&gt;secretName&lt;/code&gt; property containing the name of the &lt;a href="https://kubernetes.io/docs/concepts/configuration/secret/"&gt;Kubernetes secret&lt;/a&gt; resource that will include the files generated by cert-manager.&lt;/li&gt; &lt;li&gt;The &lt;code&gt;Issuer&lt;/code&gt; that represents the certificate authority (CA). The &lt;a href="https://dekorate.io/docs/cert-manager#issuers"&gt;Issuer section of the Dekorate documentation &lt;/a&gt; lists supported options.&lt;/li&gt; &lt;/ul&gt; &lt;p&gt;You can start with a minimal configuration in the &lt;code&gt;.properties&lt;/code&gt; file and set up the following keys:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.certificate.secret-name=tls-secret # The self-signed issuer: dekorate.certificate.self-signed.enabled=true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;a href="https://dekorate.io/configuration-guide/#cert-manager"&gt;Dekorate Cert-Manager Configuration Guide&lt;/a&gt; lists many configuration options that determine how Dekorate works with cert-manager. You can specify configuration options by adding the &lt;code&gt;@Certificate&lt;/code&gt; annotation to your Java program:&lt;/p&gt; &lt;pre&gt;&lt;code class="java"&gt;@Certificate(secretName = "tls-secret", selfSigned = @SelfSigned(enabled = true)) public class Main { // ... } &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This configuration generates all the resources in the &lt;code&gt;target/classes/dekorate/kubernetes.yml&lt;/code&gt; file, which should look like this:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;--- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: kubernetes-example spec: selfSigned: {} --- apiVersion: cert-manager.io/v1 kind: Certificate metadata: name: kubernetes-example spec: encodeUsagesInRequest: false isCA: false issuerRef: name: kubernetes-example secretName: tls-secret &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The Dekorate cert-manager extension considers the secret that contains the files generated by cert-manager and mounts it as a volume and as part of the deployment. Dekorate allows the application to access the files and configure the HTTPS/TLS endpoint:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;--- apiVersion: apps/v1 kind: Deployment metadata: name: kubernetes-example spec: replicas: 1 template: spec: containers: - name: kubernetes-example volumeMounts: - mountPath: /etc/certs name: volume-certs readOnly: true volumes: - name: volume-certs secret: optional: false secretName: tls-secret &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Securing resources&lt;/h3&gt; &lt;p&gt;When securing your resources, it's important to validate that the requests are coming from known hosts. To add these trusted hosts, use the &lt;code&gt;dnsNames&lt;/code&gt; property for the certificate configuration. The following line, for example, adds a single hostname to the property:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.certificate.dnsNames=foo.bar.com &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The certificate will then allow only requests for the &lt;code&gt;foo.bar.com&lt;/code&gt; server host. Add multiple hosts by separating them with commas.&lt;/p&gt; &lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If the DNS hostname does not exist, you will get an error.&lt;/p&gt; &lt;p&gt;In Kubernetes, you can publicly expose an application using Ingress resources, such as:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: kubernetes-example spec: rules: - host: foo.bar.com http: paths: - pathType: Prefix path: "/" backend: service: name: kubernetes-example port: number: 8080 tls: - hosts: - foo.bar.com secretName: tls-secret # &lt; cert-manager will store the created certificate in this secret. &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Dekorate can help you generate the previous Ingress resource definition by adding the following key properties:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.kubernetes.ingress.host=foo.bar.com dekorate.kubernetes.ingress.expose=true dekorate.kubernetes.ingress.tlsSecretName=tls-secret &lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Configuring HTTPS/TLS with Dekorate cert-manager extension for a Spring Boot application&lt;/h2&gt; &lt;p&gt;The example in this section demonstrates how to configure an HTTPS/TLS microservice using the Dekorate cert-manager extension.&lt;/p&gt; &lt;h3&gt;Enabling HTTPS transport&lt;/h3&gt; &lt;p&gt;To enable the HTTPS/TLS transport in Spring Boot, add the following properties:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;server.port=8443 server.ssl.enabled=true &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, configure the Java PKCS#12 keystore properties that your Spring Boot application will use to get the server certificate signed and obtain the private key:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;server.ssl.key-store-type=PKCS12 server.ssl.key-store=/path/to/keystore.p12 server.ssl.key-store-password=the password &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Since the keystore file is password protected, you need to create the secret that contains that password. This is where cert-manager comes into play. The following sections illustrate how to instruct cert-manager to generate the keystore and how to configure the application to mount and use the secret.&lt;/p&gt; &lt;h3&gt;Generating a self-signed certificate and keystore&lt;/h3&gt; &lt;p&gt;You can configure the Dekorate cert-manager extension to request the generation of the &lt;code&gt;keystore.p12&lt;/code&gt; PKCS#12 file and a self-signed certificate by specifying:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.certificate.secret-name=tls-secret dekorate.certificate.self-signed.enabled=true dekorate.certificate.keystores.pkcs12.create=true # the secret name of the password: dekorate.certificate.keystores.pkcs12.passwordSecretRef.name=pkcs12-pass dekorate.certificate.keystores.pkcs12.passwordSecretRef.key=password &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Based on this configuration, Dekorate will create the &lt;code&gt;Certificate&lt;/code&gt; and &lt;code&gt;Issuer&lt;/code&gt; resources that you can install in Kubernetes. This resource will be used by the Certificate Manager to generate a self-signed certificate and the keystore files within a secret named &lt;code&gt;tls-secret&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;To protect the keystore, create a secret named &lt;code&gt;pkcs12-pass&lt;/code&gt; in the &lt;code&gt;src/main/resources/k8s/common.yml&lt;/code&gt; file. The data field must include the key password which is encoded in base64. The following example, shows the "supersecret" string encoded in base64:&lt;/p&gt; &lt;pre&gt;&lt;code class="yaml"&gt;--- apiVersion: v1 kind: Secret metadata: name: pkcs12-pass data: # "supersecret" in base64: password: c3VwZXJzZWNyZXQ= type: Opaque &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;You can instruct Dekorate to find the &lt;code&gt;common.yml&lt;/code&gt; under &lt;code&gt;src/main/resources/&lt;/code&gt; by setting the folder name (eg. &lt;code&gt;k8s&lt;/code&gt;) in the &lt;code&gt;dekorate.options.input-path&lt;/code&gt; property:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.options.input-path=k8s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After configuring these details and installing the resources created by Dekorate on the Kubernetes platform, cert-manager will generate the generated PKCS#12 keystore file named &lt;code&gt;keystore.p12&lt;/code&gt; within the &lt;code&gt;tls-secret&lt;/code&gt; secret.&lt;/p&gt; &lt;p&gt;The Dekorate cert-manager extension will also configure the Spring Boot application to automatically mount a volume using the &lt;code&gt;tls-secret&lt;/code&gt; secret at the path &lt;code&gt;/etc/certs&lt;/code&gt; (the path can be specified using the &lt;code&gt;dekorate.certificate.volume-mount-path&lt;/code&gt; property). Therefore, you can map the Keystore file and password into the &lt;code&gt;server.ssl.key-store&lt;/code&gt; and &lt;code&gt;server.ssl.key-store-password&lt;/code&gt; Spring Boot properties:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;dekorate.kubernetes.env-vars[0].name=SERVER_SSL_KEY_STORE dekorate.kubernetes.env-vars[0].value=/etc/certs/keystore.p12 dekorate.kubernetes.env-vars[1].name=SERVER_SSL_KEY_STORE_PASSWORD dekorate.kubernetes.env-vars[1].secret=pkcs12-pass dekorate.kubernetes.env-vars[1].value=password &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Running the application in Kubernetes&lt;/h3&gt; &lt;p&gt;To run this example, you must have access to a Kubernetes cluster and &lt;a href="https://cert-manager.io/docs/installation/"&gt;install cert-manager&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Next, generate the manifests and push the application container image to your container registry. This example uses &lt;a href="https://quay.io"&gt;Quay.io&lt;/a&gt; as the container registry and &lt;code&gt;user&lt;/code&gt; as the group name, but you should substitute the values from your environment:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;mvn clean install -Ddekorate.push=true -Ddekorate.docker.registry=quay.io -Ddekorate.docker.group=user &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After you execute the previous command, install the generated manifests, which are available at &lt;code&gt;target/classes/META-INF/dekorate/kubernetes.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl apply -f target/classes/META-INF/dekorate/kubernetes.yml &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;After a few moments, check for the secret resource named &lt;code&gt;tls-secret&lt;/code&gt; created by the &lt;code&gt;Cert-Manager&lt;/code&gt; in the form of a PKCS#12 keystore file:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl get secret/tls-secret -o yaml | grep keystore.p12 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Check the status of your pods using the command:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl get pods -w &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If your application is running, the output looks like this:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;NAME READY STATUS RESTARTS AGE spring-boot-with-certmanager-example-566546987c-nj94n 1/1 Running 0 2m23s &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Try out the application by port-forwarding port 8443:&lt;/p&gt; &lt;pre&gt;&lt;code&gt;kubectl port-forward spring-boot-with-certmanager-example-566546987c-nj94n 8443:8443 &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now if you browse to &lt;code&gt;https://localhost:8443/&lt;/code&gt;, you should see &lt;code&gt;Hello world from HTTPS!&lt;/code&gt;&lt;/p&gt; &lt;h2&gt;Dekorate reduces the complexity of certification management&lt;/h2&gt; &lt;p&gt;In this article, you learned how to easily generate cert-manager custom resources using Dekorate, and how to use the generated secret by cert-manager for a Spring Boot application.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2022/07/19/secure-kubernetes-certificates-cert-manager-and-dekorate" title="Secure Kubernetes certificates with cert-manager and Dekorate"&gt;Secure Kubernetes certificates with cert-manager and Dekorate&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jose Carvajal Hilario, Ana-Maria Mihalceanu, Charles Moulliard</dc:creator><dc:date>2022-07-19T07:00:00Z</dc:date></entry></feed>
